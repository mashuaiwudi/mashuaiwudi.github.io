<!DOCTYPE html>
<html lang="en" class="shuaima"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="./file/bootstrap.min.css">
    <link rel="stylesheet" href="./file/all.min.css">
    <base href=".">
    <title>Shuai Ma | Applying for Ph.d. offer in HCI</title>
    <style>
        body {
            margin-top: 20px;
			font-family:  Helvetica;

        }

        a {
            color: black;
            border-bottom: 1px dotted black;
        }

        a:hover,
        a:active {
            color: #0099FF;
            text-decoration: none;
        }

        h1 a {
            color: #0099FF;
            border: none;
        }

        h2 {
            font-size: 1.5em;
            border-bottom: 2px solid;
            color: #0099FF;
        }
        
        h3 {
            font-size: 1.2em;
			color: #0099FF;
        }
		
        h4 {
            font-size: 1em;
			font-family:  sans-serif;
			font-weight: lighter;
			color: #0099FF;
			margin-top: 10px;
			margin-bottom: 30px;
        }

        .strong {
            color: #0099FF;
        }

        @media (max-width: 767.98px) {
            header {
                text-align: center;
            }
        }

        ul.social-icons {
            font-size: 1rem;
            margin-top: 24px;
            margin-bottom: 0;
        }

        ul.social-icons li::before {
            content: '[';
        }

        ul.social-icons li::after {
            content: ']';
        }

        ul.social-icons li a {
            border: none;
        }

        img.portrait {
            max-width: 100%;
        }

        @media (max-width: 767.98px) {
            img.portrait {
                display: block;
                max-width: 300px;
                margin: auto;
            }
        }
		
		.annotation
		{
		    margin-top: -0.5em;
		    margin-bottom: 0.5em;
		    font-size: 12px;
			line-height: 12px;
		}

        .taxonomy img {
            max-width: 100%;
        }

        div.research-project {
            font-size: 14px;
            margin-bottom: 1.5rem;
        }
		
        div.line-of-research {
            background-color:#0099FF;
        }

        div.research-project video {
            max-width: 100%;
            margin-bottom: 0.5rem;
        }

        div.research-project p {
            margin-bottom: 0.3rem;
        }

        .news {
            font-size: 15px;
            margin-bottom: 0px;
        }

        #news-more {
            display: none;
        }

        .tweets {
            overflow: auto;
            -webkit-overflow-scrolling:touch;
        }

        .fun-projects img {
            max-width: 100%;
        }
        
        .info {
            border-style: none;
            font-weight: bold;
            color: #999;
        }
		
		hr.dash{
		    border-top: 1px dashed #bbbbbb;
		    margin-bottom: 15px;
		    margin-top: 15px;
		    color: #0099FF;
		}
        
		
    </style>

	
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script type="text/javascript" async="" src="./file/analytics.js"></script><script async="" src="./file/js"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-48610112-3');
	</script>

</head>

<body data-gr-c-s-loaded="true">
    <header class="container">
        <h1 class="float-md-left mb-0">
            <a href="https://shuaima.top/">Shuai Ma | 马帅</a>
        </h1>
        <ul class="list-inline float-md-right social-icons">
            <li class="list-inline-item"><a href="index.htm">Bio</a></li>
            <li class="list-inline-item"><a href="CV_ShuaiMa.pdf">CV/Resume</a></li>
            <li class="list-inline-item"><a href="mailto:mashuai171@mails.ucas.ac.cn">Email</a></li>
        </ul>
        <div class="clearfix"></div>
        <hr>
    </header>

    <div class="container">


            <div>
                <h2>Research</h2>

                <div class="alert alert-secondary">
                    <h3>Personalization/User Modeling and Interactive Machine/Deep Learning</h3>
                    <div>
                        Every user has a difference from others. In many subjective tasks, users have significant differences and preferences. To help users do a task, a general model is usually built using ML or DL methods. However, these models are trained by data from all kinds of users, which is not suitable for a specific user. So, modeling users' personalities or preferences can be important. How to make a general model suitable for a specific user is an interesting research question.
                    </div>
                </div>

                <div class="row research-project">

                    <div class="col-md-3">
                    	<video loop muted playsinline  width="250">
                            <source type="video/mp4" src="./file/媒体1.mp4">
                        </video>
						
                    </div>
                    <div class="col-md-9">
                        <h6>
                            SmartEye: Assisting Instant Photo Taking via Integrating User Preference with Deep View Proposal Network
                        </h6>
                        <p class="text-muted">
                            Shuai Ma, Zijun Wei, Feng Tian<sup>*</sup>, Xiangmin Fan, Jianming Zhang, Xiaohui Shen, Zhe Lin, Jin Huang, Radomír Měch, Dimitris Samaras, Hongan Wang (CHI 2019)
                            <a class="info" href="paper/SmartEye.pdf">[PDF]</a>
                        </p>
                        <p class="strong">
                            <img src="./file/hm.png" width="20"> Honorable Mention Award
                        </p>
                        <p>
                            Taking a high-quality photo needs composition skill which non-expert users lack. We present SmartEye, a novel mobile system to help users take photos with good compositions in-situ. SmartEye integrates the View Proposal Network (VPN), a deep learning-based model that outputs composition suggestions in real-time, and a novel, interactively updated module (P-Module) that adjusts the VPN outputs to account for personalized composition preferences.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-3">
                        <img src="./file/research/pbayes.png" width="200">
                    </div>
                    <div class="col-md-9">
                        <h6>
                            User Adaptive Modeling in 2D Moving Target Selection 
                        </h6>
                        <p class="text-muted">
                        </p>
                        <p>
                            Our previous work has proposed a method to help 2D moving target selection. However, it used a general model trained by all users' data collected. When a new user uses our tool, it may be not suitable for him. So we designed a personalized user modeling method to adapt the general model to a specific user.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="alert alert-secondary">
                    <h3>Human-computer interaction in Healthcare</h3>
                    <div>
                        Many early stages of the disease are difficult to detect, but some symptoms can be sensitively captured by sensors. So we developed some methods of human-computer interaction to capture users' limb movement through mobile phones, Kinect and other devices, and collect a large number of data of normal users and patients, so as to achieve an assistant diagnosis of diseases.
                    </div>
                </div>

                <div class="row research-project">

                    <div class="col-md-3">
                        <img src="./file/research/pd.png" width="200">
                    </div>
                    <div class="col-md-9">
                        <h6>
                            Implicit Detection of Motor Impairment in Parkinson’s Disease from Everyday Smartphone Interactions 
                        </h6>
                        <p class="text-muted">
                            Jing Gao, Feng Tian<sup>*</sup>, Junjun Fan, Dakuo Wang, Xiangmin Fan, Yicheng Zhu, Shuai Ma, Jin Huang, Hongan Wang (CHI 2018 Poster)
                            <a class="info" href="paper/pd.pdf">[PDF]</a>
                        </p>
                        <p>
                            In this work, we explored the feasibility and accuracy of detecting motor impairment in Parkinson’s disease (PD) via implicitly sensing and analyzing users’ everyday interactions with their smartphones. Through a 42 subjects study, our approach achieved an overall accuracy of 88.1% (90.0%/86.4% sensitivity/specificity) in discriminating PD subjects from age-matched healthy controls. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-3">
                        <video loop muted playsinline  width="250">
                            <source type="video/mp4" src="./file/demo.mp4">
                        </video>
                    </div>
                    <div class="col-md-9">
                        <h6>
                            Identifying Gait Abnormality with a Single Click
                        </h6>
                        <p class="text-muted">
                            Jin Huang, Shuai Ma, Feng Tian<sup>*</sup>, Xiang Li, Jie Liu, Hongan Wang (SCIENCE CHINA)
                            <a class="info" href="paper/scis_MOOP.pdf">[PDF]</a>
                        </p>
                        <p>
                            Gait abnormality is one of the major symptoms of nervous system diseases such as Parkinson’s disease. In the clinic, assessments tools usually require patients to complete a long and tedious testing process under the supervision of a doctor, which is tremendous pressure for both patients and hospitals. We propose a novel system, which integrates identity recognition algorithm, behavior recognition algorithm, and built-in gait detection model to accelerate the clinical diagnosis process.
                        </p>
                    </div>
                </div>

                <hr class="dash">
                
                <div class="row research-project">

                    <div class="col-md-3">
                        <img src="./file/research/pinggu.png" width="200">
                    </div>
                    <div class="col-md-9">
                        <h6>
                            Human-AI Interaction in Healthcare: Three Case Studies About How Patient(s) And Doctors Interact with AI in a Multi-Tiers Healthcare Network
                        </h6>
                        <p class="text-muted">
                            Yunzhi Li, Liuping Wang, Shuai Ma, Xiangmin Fan, Zijun Wang, Junfeng Jiao, Dakuo Wang (CHI 2019 Workshop)
                            <a class="info" href="paper/chi19workshop.pdf">[PDF]</a>
                        </p>
                        <p>
                            We presents three ongoing research projects that aim to study how to design, develop, and evaluate the systems supporting human-AI interaction in the healthcare domain. Collaborating with the local government administrators, hospitals, clinics and doctors, we get a valuable opportunity to study and improve how AI-empowered technologies are changing people's life in providing or receiving healthcare services in a suburb district in Beijing, China. We hope this work will ground the discussion with other participants in the workshop and build further collaborations with the health informatics community.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="alert alert-secondary">
                    <h3>Video Interaction</h3>
                    <div>
                        To further utilize the video resources, we developed some interactive methods for video viewing, video editing, MOOC learning, etc.
                    </div>
                </div>

                <div class="row research-project">

                    <div class="col-md-3">
                    	<video loop muted playsinline poster="./file/research/pre-screen.png">
                            <source type="video/mp4" src="./file/uist19b-sub1063-cam-i16.mp4">
                        </video>
                    </div>
                    <div class="col-md-9">
                        <h6>
                            Pre-screen: Assisting Material Screening in Early-stage of Video Editing
                        </h6>
                        <p class="text-muted">
                            Qian Zhu and Shuai Ma<sup>*</sup> (UIST 2019 Poster)
                            <a class="info" href="paper/pre-screen.pdf">[PDF]</a>
                        </p>

                        <p>
                            In video editing, screening raw material is a time-consuming step. We present Pre-screen, which integrates several deep learning approaches to assist users to screen materials in the early-stage of video editing. We did a preliminary study on users in the video editing domain and based on the result, we well designed and implemented Pre-screen which provides 4 supported functions to better help users screen vide materials.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">
                    <div class="col-md-3">
                        <img src="./file/research/reminder.png" width="200">
                    </div>
                    <div class="col-md-9">
                        <h6>
                            What Did I Miss? Assisting User-adaptive Missed Content Reviewing in MOOC Learning
                        </h6>
                        <p class="text-muted">
                            Qian Zhu and Shuai Ma<sup>*</sup> (UIST 2019 Poster)
                            <a class="info" href="paper/reminder.pdf">[PDF]</a>
                        </p>
                            In MOOC learning, learners can easily get distracted as there is no real teacher monitoring them. We present Reminder, a tool to monitor learners' attention and help them review the missed content in the lecture. The attention monitoring function only relies on a front-camera in pc, tablet, pad or phone. Reminder provides a multi-scale attention score viewing method from which users can easily know which part they have missed.
                        <p>

                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-3">
                        <img src="./file/research/co-lighter.png" width="200">
                    </div>
                    <div class="col-md-9">
                        <h6>
                            Co-Lighter: Promoting Video Watching by Crowd Suggestion on Specific Content
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="paper/colighter.pdf">[PDF]</a>
                        </p>
                        <p>
                            In this research, we first conduct a preliminary survey among 114 participants to investigate the limits in current video watching modes. Then, based on the survey results, we present Co-Lighter, a novel tool for video viewing and comment. Co-Lighter supports viewers to watch videos and share feelings about video content in a collaborative way. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-3">
                        <img src="./file/research/e-storytelling.png" width="200">
                    </div>
                    <div class="col-md-9">
                        <h6>
                            E-StorySpace: Explore Movie Story with Emotion in Space
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="paper/espace.pdf">[PDF]</a>
                        </p>
                        <p>
                            E-StorySpace is a spatial visualization for understanding and exploring the story in movies. We first automatically extracted a series of elements of a story such as scenes, characters, and dialogues with a movie script processing algorithms. And we extracted multi-modal emotional features of a movie by a deep neural network. Then we organized these elements of a story into feature vectors and clustered them at different levels in 2D space. So we got the spatial distribution of story content that can help users to explore and compare different parts of a story that they are interested in. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                

                <div class="alert alert-secondary">
                    <h3>Interaction Technique for Daily Life</h3>
                    <div>
                        Interaction is everywhere in our daily life. What can it do to create a better life?
                    </div>
                </div>

                <div class="row research-project">

                    <div class="col-md-3">
                        <img src="./file/research/mirroru.png" width="200">
                    </div>
                    <div class="col-md-9">
                        <h6>
                            mirrorU: Scaffolding Emotional Reflection via In-Situ Assessment and Interactive Feedback
                        </h6>
                        <p class="text-muted">
                            Liuping Wang, Xiangmin Fan<sup>*</sup>, Feng Tian, Lingjia Deng, Shuai Ma, Jin Huang, Hongan Wang (CHI 2018 Poster)
                            <a class="info" href="paper/mirrorU.pdf">[PDF]</a>
                        </p>
                        <p>
                            We present mirrorU, a mobile system that supports users to reflect on and write about their daily emotional experience. While prior work has focused primarily on providing memory triggers or affective cues, mirrorU provides in-situ assessment and interactive feedback to scaffold reflective writing. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="alert alert-secondary">
                    <h3>Interaction Technique for Touch and Design</h3>
                    <div>
                        We proposed some systems for design and gesture recognition.
                    </div>
                </div>

                <div class="row research-project">

                    <div class="col-md-3">
                        <img src="./file/research/furniture.png" width="200">
                    </div>
                    <div class="col-md-9">
                        <h6>
                            Upcycle-Chic: A Software Tool for Ideating Furniture Upcycling Design
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="paper/upcycle.pdf">[PDF]</a>
                        </p>
                        <p>
                            We present Upcycle-Chic, a design and visualization environment that allows a user to view possible upcycling solutions for a given piece of old furniture and explore varying design variations. These possible solutions are generated based on design strategies drawn from over 1000 examples on the web and books shared by professionals and hobbyist furniture makers.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-3">
                        <img src="./file/research/recognizer.png" width="200">
                    </div>
                    <div class="col-md-9">
                        <h6>
                            Chronos: Improving Recognizers’ Performance by Leveraging Gesture Continuity and Designers’ Involvement
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="paper/recognizer.pdf">[PDF]</a>
                        </p>
                        <p>
                            We present Chronos, an algorithm framework that improves the performance of gesture recognizers by 1) extracting the continuity information from gesture sequences and, 2) enabling designers to optimize the decision-making rewards. The framework is implemented by integrating a dynamic Bayesian network (DBN) with a partially observable Markov decision process (POMDP).
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="alert alert-secondary">
                    <h3>Human Engagement and Trust with AI</h3>
                    <div>
                        When collaborating with AI, how will users feel and do they trust AI? To investigate these questions, we did some interesting research.
                    </div>
                </div>

                <div class="row research-project">

                    <div class="col-md-3">
                        <img src="./file/research/preg.png" width="200">
                    </div>
                    <div class="col-md-9">
                        <h6>
                            Investigating Pregnent Women' Engagement When Getting Emotion Support from a Chatbot
                        </h6>
                        <p class="text-muted">
                        </p>
                        <p>
                            Pregnant women are seeking emotional support in forums. How would they feel if they were replied by a chatbot? We are investigating users’ engagement when getting emotional support in a pregnant related forum where users don’t know whether the comments are replied by a real user or a chatbot. To build the chatbot, we designed a seq-to-seq model to generate diverse comments based on posts. We Hide the robot in the community to reply to the user's posts, and then evaluated users' engagement.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                

                


            </div>
            

        </div>
    </div>

<script src="./file/jquery.min.js"></script>
<script>
    $('#toggle-more-news').click(function () {
        $('#news-more').toggle();
        $('#news-more').is(':visible') ? $(this).text('< Hide') : $(this).text('More >');
        return false;
    });

    $(window).on('scroll', function () {
        $('video').each(function () {
            var video = $(this)[0];
            var rect = video.getBoundingClientRect();

            if (
                rect.top >= 0 && rect.left >= 0 &&
                rect.bottom <= $(window).height() &&
                rect.right <= $(window).width()
            ) {
                video.play();
            } else {
                video.pause();
            }
        });
    });
</script>

</body></html>