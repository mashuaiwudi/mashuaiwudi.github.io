<!DOCTYPE html>
<!-- saved from url=(0022)https://yangzhang.dev/ -->
<html lang="en" class="gr__yangzhang_dev"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="./Yang Zhang _ Sensing research at CMU_files/bootstrap.min.css">
    <link rel="stylesheet" href="./Yang Zhang _ Sensing research at CMU_files/all.min.css">
    <!--<base href="https://yangzhang.dev/">--><base href=".">
    <title>Shuai Ma | Applying for Ph.d. offer in HCI</title>
    <style>
        body {
            margin-top: 20px;
			font-family:  sans-serif;
			font-weight: lighter;

        }

        a {
            color: black;
            border-bottom: 1px dotted black;
        }

        a:hover,
        a:active {
            color: #DB522F;
            text-decoration: none;
        }

        h1 a {
            color: #6B747C;
            border: none;
        }

        h2 {
            font-size: 1.5em;
            border-bottom: 2px solid;
        }
        
        h3 {
            font-size: 1.2em;
			color: #000000;
        }
		
        h4 {
            font-size: 1em;
			font-family:  sans-serif;
			font-weight: lighter;
			color: #000000;
			margin-top: 10px;
			margin-bottom: 30px;
        }

        .strong {
            color: #DB522F;
        }

        @media (max-width: 767.98px) {
            header {
                text-align: center;
            }
        }

        ul.social-icons {
            font-size: 1rem;
            margin-top: 24px;
            margin-bottom: 0;
        }

        ul.social-icons li::before {
            content: '[';
        }

        ul.social-icons li::after {
            content: ']';
        }

        ul.social-icons li a {
            border: none;
        }

        img.portrait {
            max-width: 100%;
        }

        @media (max-width: 767.98px) {
            img.portrait {
                display: block;
                max-width: 300px;
                margin: auto;
            }
        }
		
		.annotation
		{
		    margin-top: -0.5em;
		    margin-bottom: 0.5em;
		    font-size: 12px;
			line-height: 12px;
		}

        .taxonomy img {
            max-width: 100%;
        }

        div.research-project {
            font-size: 14px;
            margin-bottom: 1.5rem;
        }
		
        div.line-of-research {
            background-color:#F0F0F0;
        }

        div.research-project video {
            max-width: 100%;
            margin-bottom: 0.5rem;
        }

        div.research-project p {
            margin-bottom: 0.3rem;
        }

        .news {
            font-size: 15px;
            margin-bottom: 0px;
        }

        #news-more {
            display: none;
        }

        .tweets {
            overflow: auto;
            -webkit-overflow-scrolling:touch;
        }

        .fun-projects img {
            max-width: 100%;
        }
        
        .info {
            border-style: none;
            font-weight: bold;
            color: #999;
        }
		
		hr.dash{
		    border-top: 1px dashed #bbbbbb;
		    margin-bottom: 15px;
		    margin-top: 15px;
		}
        
		
    </style>
    <link rel="apple-touch-icon" sizes="180x180" href="https://moon.feitsui.com/img/zhangyang/180x180.png">
    <link rel="icon" sizes="192x192" href="https://moon.feitsui.com/img/zhangyang/192x192.png" type="image/png">
	
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script type="text/javascript" async="" src="./Yang Zhang _ Sensing research at CMU_files/analytics.js"></script><script async="" src="./Yang Zhang _ Sensing research at CMU_files/js"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-48610112-3');
	</script>

</head>

<body data-gr-c-s-loaded="true">
    <header class="container">
        <h1 class="float-md-left mb-0">
            <a href="https://yangzhang.dev/">Shuai Ma</a>
        </h1>
        <div class="clearfix"></div>
        <hr>
    </header>

    <div class="container">
        <div class="row mb-3">
            <div class="col-xl-8 col-lg-9 col-md-8">
                <p>
                    Shuai Ma is a 3rd (final) year Master Degree candidate at Human-Computer Interaction Lab, Institute of Software, Chinese Academy of Sciences (ISCAS), advised by Prof. <a href="http://people.ucas.ac.cn/~0007889">Feng Tian</a>.
                </p>
                <p>
					His primary research focuses on AI-supported HCI and Human-engaged AI. His research interests include User Modeling, AI collaborate with Human and their applications in Healthcare, Media Interaction, Education. He received his Bachelor degree in Computer Science from Harbin Institute of Technology (HIT).
                </p>
				
                <p>
					He publishes at <a href="https://dl.acm.org/event.cfm?id=RE151">CHI</a> (ACM CHI Conference on Human Factors in Computing Systems) and <a href="https://dl.acm.org/event.cfm?id=RE172">UIST</a> (ACM Symposium on User Interface Software and Technology), and has received 1 Honorable Mention Awards (5%).
                </p>
                <p>
                    Shuai is applying for a Ph.d. offer in HCI.
                </p>
            </div>
            <div class="col-lg-3 col-md-4">
                <img src="./Yang Zhang _ Sensing research at CMU_files/me_portrait.jpg" width="350">
            </div>
        </div>



        <div class="row">
            <!-- left column -->
            <div class="col-lg-8 mb-2">
                <h2>Research</h2>


                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/pre-screen.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Pre-screen: Assisting Material Screening in Early-stage of Video Editing
                        </h6>
                        <p class="text-muted">
                            Qian Zhu and Shuai Ma<sup>*</sup> (UIST 2019 Poster)
							<a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>

                        <p>
                            In video editing, screening raw material is a time-consuming step. We present Pre-screen, which integrates several deep learning approaches to assist users to screen materials in the early-stage of video editting. We did a preliminary study on users in the video editing domain and based on the result, we well designed and implemented Pre-screen which provides 4 supported fuctions to better help users screen vide materials.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">
                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/reminder.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            What Did I Miss? Assisting User-adaptive Missed Content Reviewing in MOOC Learning
                        </h6>
                        <p class="text-muted">
                            Qian Zhu and Shuai Ma<sup>*</sup> (UIST 2019 Poster)
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3173847">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Wall/Wall.pdf">[PDF]</a>
                        </p>
                            In MOOC learning, learners can easily get distracted as there is no real teacher monitoring them. We present Reminder, a tool to monitor learners' attention and help them review the missed content in lecture. The attention monitoring function only rely on a front-camera in pc, tablet, pad or phone. Reminder provides a multi-scale attention score viewing method from which users can easily know which part they have missed.
                        <p>

                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/smarteye.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            SmartEye: Assisting Instant Photo Taking via Integrating User Preference with Deep View Proposal Network
                        </h6>
                        <p class="text-muted">
                            Shuai Ma, Zijun Wei, Feng Tian<sup>*</sup>, Xiangmin Fan, Jianming Zhang, Xiaohui Shen, Zhe Lin, Jin Huang, Radomír Měch, Dimitris Samaras, Hongan Wang (CHI 2019)
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p class="strong">
                            <img src="./Yang Zhang _ Sensing research at CMU_files/hm.png" width="20"> Honorable Mention Award
                        </p>
                        <p>
                            Taking a high-quality photo needs composition skill which non-expert users lack. a novel mobile system to help users take photos with good compositions in-situ. SmartEye integrates the View Proposal Network (VPN), a deep learning based model that outputs composition suggestions in real time, and a novel, interactively updated module (P-Module) that adjusts the VPN outputs to account for personalized composition preferences.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/pd.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Implicit Detection of Motor Impairment in Parkinson’s Disease from Everyday Smartphone Interactions 
                        </h6>
                        <p class="text-muted">
                            Jing Gao, Feng Tian<sup>*</sup>, Junjun Fan, Dakuo Wang, Xiangmin Fan, Yicheng Zhu, Shuai Ma, Jin Huang, Hongan Wang (CHI 2018 Poster)
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            In this work, we explored the feasibility and accuracy of detecting motor impairment in Parkinson’s disease (PD) via implicitly sensing and analyzing users’ everyday interactions with their smartphones. Through a 42 subjects study, our approach achieved an overall accuracy of 88.1% (90.0%/86.4% sensitivity/specificity) in discriminating PD subjects from age-matched healthy controls. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/mirroru.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            mirrorU: Scaffolding Emotional Reflection via In-Situ Assessment and Interactive Feedback
                        </h6>
                        <p class="text-muted">
                            Liuping Wang, Xiangmin Fan<sup>*</sup>, Feng Tian, Lingjia Deng, Shuai Ma, Jin Huang, Hongan Wang (CHI 2018 Poster)
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            We present mirrorU, a mobile system that supports users to reflect on and write about their daily emotional experience. While prior work has focused primarily on providing memory triggers or affective cues, mirrorU provides in-situ assessment and interactive feedback to scaffold reflective writing. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/furniture.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Upcycle-Chic: A Software Tool for Ideating Furniture Upcycling Design
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            We present Upcycle-Chic, a design and visualization environment that allows a user to view possible upcycling solutions for a given piece of old furniture and explore varying design variations. These possible solutions are generated based on design strategies drawn from over 1000 examples on web and books shared by professionals and hobbyist furniture makers.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/gait.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Identifying Gait Abnormality in Nervous System Diseases with a Single Click}{Title for citation
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            Gait abnormality is one of the major symptoms of nervous system diseases such as Parkinson’s disease. In clinic, assessments tools usually requires patients to complete a long and tedious testing process under the supervision of a doctor, which is a tremendous pressure for both patients and hospitals.We propose a novel system, which integrates identity recognition algorithm, behavior recognition algorithm, and built-in gait detection model to accelerate the clinical diagnosis process.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/co-lighter.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Co-Lighter: Promoting Video Watching by Crowd Suggestion on Specific Content
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            We first conduct a preliminary survey among 114 participants to investigate the limits in current video watching modes. Then, based on the survey results, we present Co-Lighter, a novel tool for video viewing and comment. Co-Lighter supports viewers to watch videos and share feelings about video content in a collaborative way. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/e-storytelling.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            E-StorySpace: Explore Movie Story with Emotion in Space
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            E-StorySpace is a spatial visualization for understanding and exploring story in movies. We first automatically extracted a series of elements of a story such as scenes, characters and dialogues with movie script processing algorithm. And we extracted multi-modal emotional features of a movie by deep neural network. Then we organized these elements of a story into feature vectors and clustered them at different levels in 2D space. So we got the spatial distribution of story content that can help users to explore and compare different parts of a story that they are interested in. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/preg.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Investigating Pregnent Women' Engagement When Getting Emotion Support from a Chatbot
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            Pregnant women are seeking for emotional support in forums. How would they feel if they were replied by a chatbot? We are investigating users’ engagement when getting emotional support in a pregnant related forum where users don’t know whether the com-ments are replied by a real user or a chatbot. To support the chatbot, we built a seq-to-seq model to generate diverse comments based on posts
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/pbayes.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            User Adaptive Modeling in 2D Moving Target Selection 
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            Our previous work has proposed a method to help 2D moving target selection. However, it used a general model trained by all users' data collected. When a new users use our tool, it may be not suitable for him. So we designed a personalized user modeling method to adapt the general model to a specific user.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/recognizer.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Chronos: Improving Recognizers’ Performance by Leveraging Gesture Continuity and Designers’ Involvement
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            We presents Chronos, an algorithm framework that improves the performance of gesture recognizers by 1) extracting the continuity information from gesture sequences and, 2) enabling designers to optimize the decision-making rewards. The framework is implemented by integrating dynamic Bayesian network (DBN) with a partially observable Markov decision process (POMDP).
                        </p>
                    </div>
                </div>


            </div>
            <!-- /left column -->
            <!-- right column -->
            <div class="col-lg-4 mb-2">
                <h2>Latest News</h2>
                <ul class="news">
					<li>Aug 2 Two poster abstracts got conditionally accepted at UIST 2019.</li>
					<li>May 9 Give a talk at CHI 2019 conference.</li>
					<li>Mar 15 One paper got CHI Honorable Mention Award!</li>
					
                </ul>
                <ul class="news" id="news-more">
                    <li>Dec 22 Visit <a href="http://www.a-su.com.cn/">ASU</a> and Ling.</li>
                    <li>Dec 17 Visit Hong Kong to see my wife.</li>
                    <li>Oct 20 Back in Pittsburgh.</li>
                    <li>Oct 16 Attend UIST 2016 @ Tokyo, give AuraSense presentation.</li>
                    <li>Oct 09 Talk about research and share experience living abroad with <a href="http://dsd.future-lab.cn/">ICMLL
                            lab</a>.</li>
                    <li>Oct 06 Wonderful wedding ceremony with two families and friends at Beijing.</li>
                    <li>Sep 22 post-CHI party at Union Grill. Preparing for my wedding.</li>
                    <li>Aug 28 CHI 2017 projects final push.</li>
                    <li>Jun 26 Three lab papers got accepted at UIST 2016. Go FIGlab!</li>
                    <li>Jun 1 Summer projects for CHI 2017 are in full swing.</li>
                    <li>May 16 I got married!</li>
                    <li>Apr 13 UIST 2016 Paper submitted. Fly to St. Louis for weekends.</li>
                    <li>Apr 1 UIST 2016 in full swing. </li>
                    <li>Mar 23 Qualcomm Innovation Fellowship finalist presentation and demo. </li>
                    <li>Jan 26 Pittsburgh Penguins vs. New Jersey Devils. We won! </li>
                    <li>2016 - Jan 9 Got back to Pittsburgh. New semester started! </li>
                    <li>Dec 16 Filming for CHI project done. Flying back to Beijing.</li>
                    <li>Nov 30 CHI rebuttals submitted.</li>
                    <li>Nov 26 Host friends from high school over thanksgiving.</li>
                    <li>Nov 11 Reunion dinner with CoDelab friends. Wonderful UIST2015.</li>
                    <li>Nov 7 Heading for UIST 2015, Charlotte, NC. </li>
                    <li>Oct 28 Demo at Engedget, NYC. </li>
                    <li>Oct 26 Received a happy birthday suprise from the lab. </li>
                    <li>Sep 12 CHI 2016 projects final push. </li>
                    <li>Aug 31 First day as a PhD student. </li>
                    <li>Aug 23 Summer project user study began. </li>
                    <li>Aug 20 Tomo and Quantifying Electrostatic Haptic Feedback got accepted by UIST and ITS
                        2015. </li>
                    <li>Jun 3 Summer projects in full swing. </li>
                    <li>May 20 Tour at DC with family. </li>
                    <li>Apr 15 Party after UIST submission at Butter Joint. </li>
                    <li>Apr 7 UIST 2015 in full swing. </li>
                    <li>Mar 24 Make food storage in the lab for UIST late night work. </li>
                    <li>Mar 21 Had a wonderful visit at Cornell Tech NYC and Ithaca. </li>
                    <li>Feb 14 Extreme cold weather in St Louis. </li>
                    <li>Jan 21 User test for the Fitts Law Project. 10 down, 10 to go! </li>
                    <li>Jan 13 Back at Pittsburgh. </li>
                    <li>2015 - Jan 5 V1.0 bio-impedance meter board is sent for printing. </li>
                    <li>Dec 14 Went back home. Happy birthday mom! </li>
                    <li>Oct 9 ACM UIST conference Student Innovation Contest 1st Most Creative Award for our
                        project!</li>
                    <li>Aug 22 Finished my internship at Kinoma, Marvell. </li>
                    <li>Jun 18 Won the first place in IoT Hackathon, evironment category. </li>
                    <li>May 29 The third day as intern. Developed an alarm using openweathermap and Google TTS API.
                    </li>
                    <li>May 25 Arrive at Santa Clara for the summer intern. </li>
                    <li>May 07 Final exam of 15213, done with high score. </li>
                    <li>Apr 23 Final presentation of ZipperSense. </li>
                    <li>Apr 18 Travel to Phily, play basketball with friends. </li>
                    <li>Apr 04 V3.0 PCB board for the ZipperSense is sent for printing. </li>
                    <li>Mar 07 V2.0 PCB board for the ZipperSense is sent for printing. </li>
                    <li>Feb 21 V1.0 PCB board for the final project of gadget class is being printed. </li>
                    <li>Jan 22 Turned an old cushion and a box into a stray cat's nest.</li>
                    <li>Jan 20 30-minutes running, 1st day. The goal is to beats the number of days last semester.</li>
                    <li>Jan 13 New semester begins. I'm so excited.</li>
                    <li>Jan 11 Back to Pittsburgh.</li>
                    <li>Jan 05 St Louis snow storm. Store food and water.</li>
                    <li>2014 - Jan 01 Went to St Louis to spend the first day of the new year with my girl friend.</li>


                    <li>Dec 27 Went to The Grand Canyon which is truly grand.</li>
                    <li>Dec 24 Went to Las Vegas.</li>
                    <li>Dec 02 Successful presentation of 24780 C++ class's final project--"Interactive Fish"</li>
                    <li>Nov 28 Have a big Thanks-giving Turkey dinner in Jake's parents' house.</li>
                    <li>Nov 14 Paper and video finished for TEI. Everyday 1000-yards-swimming over 66 days!</li>
                    <li>Nov 05 Prototype1 of Heart Pulse is completed. Busy finishing the work for TEI conference.</li>
                    <li>Oct 21 Three final projects proposal. I'm really excited to get start.</li>
                    <li>Oct 9 Top-5 program in C++ class. Reward is bowling bowl game tickets!</li>
                    <li>Oct 6 Everyday 1000-yards-swimming, over 28 days.</li>
                    <li>Oct 1 Perform well in C++ python midterm. Finally make our silicon mask work with
                        solenoids.</li>
                    <li>Sep 21 Everyday 1000-yards-swimming, over 13 days.</li>
                    <li>Sep 9 Visit Prof. Dale's house. Having fun.</li>
                    <li>Aug 30 First week in CMU. Cool! Now going to St. Louis for weekend</li>
                    <li>Aug 13 Orientation as new graduated student. End with a hugh BBQ!</li>
                    <li>Aug 8 Arrive at Pittsburg.</li>
                    <li>Jul 28 Bought one pair of hiking shoes.</li>
                    <li>Jul 21 Pack up stuff for studing abroad.</li>
                    <li>Jul 20 Send our pet dog to school.</li>
                    <li>Jul 19 Change domain name to "bennyzhang.me"</li>
                    <li>Jul 7 Return to Beijing.</li>
                    <li>Jul 2 Arrive at Liaoyuan.</li>
                    <li>Jun 27 Best barbecue I ever had. Also had the local spicy soup and noodle. Super spicy!</li>
                    <li>Jun 25 Depart from my dorm. Head for Changchun, my roommate's hometown.</li>
                    <li>2013 - Jun 15 Pack up my stuff. Only ten days to leave my Beihang University.</li>
                </ul>
                <a id="toggle-more-news" href="https://yangzhang.dev/#">More &gt;</a>

            </div>
            <!-- /right column -->
        </div>

        <h2>Fun Life</h2>
        <div class="row fun-projects">
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://github.com/yangz3/DOTA2-Player-Survival-Rate-Analysis">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/1.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/2.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="./Yang Zhang _ Sensing research at CMU_files//life/3.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/4.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/5.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/6.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://youtu.be/5Fge4-N5Avo">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/7.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://youtu.be/H70mu4YS5oo">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/8.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <a href="https://youtu.be/H70mu4YS5oo">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/9.jpg"></a>
            </div>
        </div>
    </div>

<script src="./Yang Zhang _ Sensing research at CMU_files/jquery.min.js"></script>
<script>
    $('#toggle-more-news').click(function () {
        $('#news-more').toggle();
        $('#news-more').is(':visible') ? $(this).text('< Hide') : $(this).text('More >');
        return false;
    });

    $(window).on('scroll', function () {
        $('video').each(function () {
            var video = $(this)[0];
            var rect = video.getBoundingClientRect();

            if (
                rect.top >= 0 && rect.left >= 0 &&
                rect.bottom <= $(window).height() &&
                rect.right <= $(window).width()
            ) {
                video.play();
            } else {
                video.pause();
            }
        });
    });
</script>

</body></html>