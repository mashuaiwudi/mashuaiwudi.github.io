<!DOCTYPE html>
<!-- saved from url=(0022)https://yangzhang.dev/ -->
<html lang="en" class="gr__yangzhang_dev"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link rel="stylesheet" href="./Yang Zhang _ Sensing research at CMU_files/bootstrap.min.css">
    <link rel="stylesheet" href="./Yang Zhang _ Sensing research at CMU_files/all.min.css">
    <!--<base href="https://yangzhang.dev/">--><base href=".">
    <title>Shuai Ma | Applying for Ph.d. offer in HCI</title>
    <style>
        body {
            margin-top: 20px;
			font-family:  sans-serif;
			font-weight: lighter;

        }

        a {
            color: black;
            border-bottom: 1px dotted black;
        }

        a:hover,
        a:active {
            color: #DB522F;
            text-decoration: none;
        }

        h1 a {
            color: #6B747C;
            border: none;
        }

        h2 {
            font-size: 1.5em;
            border-bottom: 2px solid;
        }
        
        h3 {
            font-size: 1.2em;
			color: #000000;
        }
		
        h4 {
            font-size: 1em;
			font-family:  sans-serif;
			font-weight: lighter;
			color: #000000;
			margin-top: 10px;
			margin-bottom: 30px;
        }

        .strong {
            color: #DB522F;
        }

        @media (max-width: 767.98px) {
            header {
                text-align: center;
            }
        }

        ul.social-icons {
            font-size: 1rem;
            margin-top: 24px;
            margin-bottom: 0;
        }

        ul.social-icons li::before {
            content: '[';
        }

        ul.social-icons li::after {
            content: ']';
        }

        ul.social-icons li a {
            border: none;
        }

        img.portrait {
            max-width: 100%;
        }

        @media (max-width: 767.98px) {
            img.portrait {
                display: block;
                max-width: 300px;
                margin: auto;
            }
        }
		
		.annotation
		{
		    margin-top: -0.5em;
		    margin-bottom: 0.5em;
		    font-size: 12px;
			line-height: 12px;
		}

        .taxonomy img {
            max-width: 100%;
        }

        div.research-project {
            font-size: 14px;
            margin-bottom: 1.5rem;
        }
		
        div.line-of-research {
            background-color:#F0F0F0;
        }

        div.research-project video {
            max-width: 100%;
            margin-bottom: 0.5rem;
        }

        div.research-project p {
            margin-bottom: 0.3rem;
        }

        .news {
            font-size: 15px;
            margin-bottom: 0px;
        }

        #news-more {
            display: none;
        }

        .tweets {
            overflow: auto;
            -webkit-overflow-scrolling:touch;
        }

        .fun-projects img {
            max-width: 100%;
        }
        
        .info {
            border-style: none;
            font-weight: bold;
            color: #999;
        }
		
		hr.dash{
		    border-top: 1px dashed #bbbbbb;
		    margin-bottom: 15px;
		    margin-top: 15px;
		}
        
		
    </style>
    <link rel="apple-touch-icon" sizes="180x180" href="https://moon.feitsui.com/img/zhangyang/180x180.png">
    <link rel="icon" sizes="192x192" href="https://moon.feitsui.com/img/zhangyang/192x192.png" type="image/png">
	
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script type="text/javascript" async="" src="./Yang Zhang _ Sensing research at CMU_files/analytics.js"></script><script async="" src="./Yang Zhang _ Sensing research at CMU_files/js"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-48610112-3');
	</script>

</head>

<body data-gr-c-s-loaded="true">
    <header class="container">
        <h1 class="float-md-left mb-0">
            <a href="https://yangzhang.dev/">Shuai Ma</a>
        </h1>
        <div class="clearfix"></div>
        <hr>
    </header>

    <div class="container">
        <div class="row mb-3">
            <div class="col-xl-8 col-lg-9 col-md-8">
                <p>
                    Shuai Ma is a 3rd (final) year Master Degree candidate at Human-Computer Interaction Lab, Institute of Software, Chinese Academy of Sciences (ISCAS), advised by Prof. <a href="http://people.ucas.ac.cn/~0007889">Feng Tian</a>.
                </p>
                <p>
					His primary research focuses on AI-supported HCI and Human-engaged AI. His research interests include User Modeling, AI collaborate with Human and their applications in Healthcare, Media Interaction, Education. He received his Bachelor degree in Computer Science from Harbin Institute of Technology (HIT).
                </p>
				
                <p>
					He publishes at <a href="https://dl.acm.org/event.cfm?id=RE151">CHI</a> (ACM CHI Conference on Human Factors in Computing Systems) and <a href="https://dl.acm.org/event.cfm?id=RE172">UIST</a> (ACM Symposium on User Interface Software and Technology), and has received 1 Honorable Mention Awards (5%).
                </p>
                <p>
                    Shuai is applying for a Ph.d. offer in HCI.
                </p>
            </div>
            <div class="col-lg-3 col-md-4">
                <img src="./Yang Zhang _ Sensing research at CMU_files/me_portrait.jpg" width="350">
            </div>
        </div>



        <div class="row">
            <!-- left column -->
            <div class="col-lg-8 mb-2">
                <h2>Research</h2>


                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/pre-screen.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Pre-screen: Assisting Material Screening in Early-stage of Video Editing
                        </h6>
                        <p class="text-muted">
                            Qian Zhu and Shuai Ma<sup>*</sup> (UIST 2019 Poster)
							<a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>

                        <p>
                            In video editing, screening raw material is a time-consuming step. We present Pre-screen, which integrates several deep learning approaches to assist users to screen materials in the early-stage of video editting. We did a preliminary study on users in the video editing domain and based on the result, we well designed and implemented Pre-screen which provides 4 supported fuctions to better help users screen vide materials.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">
                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/reminder.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            What Did I Miss? Assisting User-adaptive Missed Content Reviewing in MOOC Learning
                        </h6>
                        <p class="text-muted">
                            Qian Zhu and Shuai Ma<sup>*</sup> (UIST 2019 Poster)
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3173847">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Wall/Wall.pdf">[PDF]</a>
                        </p>
                            In MOOC learning, learners can easily get distracted as there is no real teacher monitoring them. We present Reminder, a tool to monitor learners' attention and help them review the missed content in lecture. The attention monitoring function only rely on a front-camera in pc, tablet, pad or phone. Reminder provides a multi-scale attention score viewing method from which users can easily know which part they have missed.
                        <p>

                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/smarteye.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            SmartEye: Assisting Instant Photo Taking via Integrating User Preference with Deep View Proposal Network
                        </h6>
                        <p class="text-muted">
                            Shuai Ma, Zijun Wei, Feng Tian<sup>*</sup>, Xiangmin Fan, Jianming Zhang, Xiaohui Shen, Zhe Lin, Jin Huang, Radomír Měch, Dimitris Samaras, Hongan Wang (CHI 2019)
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p class="strong">
                            <img src="./Yang Zhang _ Sensing research at CMU_files/hm.png" width="20"> Honorable Mention Award
                        </p>
                        <p>
                            Taking a high-quality photo needs composition skill which non-expert users lack. a novel mobile system to help users take photos with good compositions in-situ. SmartEye integrates the View Proposal Network (VPN), a deep learning based model that outputs composition suggestions in real time, and a novel, interactively updated module (P-Module) that adjusts the VPN outputs to account for personalized composition preferences.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/pd.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Implicit Detection of Motor Impairment in Parkinson’s Disease from Everyday Smartphone Interactions 
                        </h6>
                        <p class="text-muted">
                            Jing Gao, Feng Tian<sup>*</sup>, Junjun Fan, Dakuo Wang, Xiangmin Fan, Yicheng Zhu, Shuai Ma, Jin Huang, Hongan Wang (CHI 2018 Poster)
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            In this work, we explored the feasibility and accuracy of detecting motor impairment in Parkinson’s disease (PD) via implicitly sensing and analyzing users’ everyday interactions with their smartphones. Through a 42 subjects study, our approach achieved an overall accuracy of 88.1% (90.0%/86.4% sensitivity/specificity) in discriminating PD subjects from age-matched healthy controls. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/mirroru.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            mirrorU: Scaffolding Emotional Reflection via In-Situ Assessment and Interactive Feedback
                        </h6>
                        <p class="text-muted">
                            Liuping Wang, Xiangmin Fan<sup>*</sup>, Feng Tian, Lingjia Deng, Shuai Ma, Jin Huang, Hongan Wang (CHI 2018 Poster)
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            We present mirrorU, a mobile system that supports users to reflect on and write about their daily emotional experience. While prior work has focused primarily on providing memory triggers or affective cues, mirrorU provides in-situ assessment and interactive feedback to scaffold reflective writing. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/furniture.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Upcycle-Chic: A Software Tool for Ideating Furniture Upcycling Design
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            We present Upcycle-Chic, a design and visualization environment that allows a user to view possible upcycling solutions for a given piece of old furniture and explore varying design variations. These possible solutions are generated based on design strategies drawn from over 1000 examples on web and books shared by professionals and hobbyist furniture makers.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/gait.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Identifying Gait Abnormality in Nervous System Diseases with a Single Click}{Title for citation
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            Gait abnormality is one of the major symptoms of nervous system diseases such as Parkinson’s disease. In clinic, assessments tools usually requires patients to complete a long and tedious testing process under the supervision of a doctor, which is a tremendous pressure for both patients and hospitals.We propose a novel system, which integrates identity recognition algorithm, behavior recognition algorithm, and built-in gait detection model to accelerate the clinical diagnosis process.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/co-lighter.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Co-Lighter: Promoting Video Watching by Crowd Suggestion on Specific Content
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            We first conduct a preliminary survey among 114 participants to investigate the limits in current video watching modes. Then, based on the survey results, we present Co-Lighter, a novel tool for video viewing and comment. Co-Lighter supports viewers to watch videos and share feelings about video content in a collaborative way. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/e-storytelling.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            E-StorySpace: Explore Movie Story with Emotion in Space
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            E-StorySpace is a spatial visualization for understanding and exploring story in movies. We first automatically extracted a series of elements of a story such as scenes, characters and dialogues with movie script processing algorithm. And we extracted multi-modal emotional features of a movie by deep neural network. Then we organized these elements of a story into feature vectors and clustered them at different levels in 2D space. So we got the spatial distribution of story content that can help users to explore and compare different parts of a story that they are interested in. 
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/preg.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Investigating Pregnent Women' Engagement When Getting Emotion Support from a Chatbot
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            Pregnant women are seeking for emotional support in forums. How would they feel if they were replied by a chatbot? We are investigating users’ engagement when getting emotional support in a pregnant related forum where users don’t know whether the com-ments are replied by a real user or a chatbot. To support the chatbot, we built a seq-to-seq model to generate diverse comments based on posts
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/pbayes.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            User Adaptive Modeling in 2D Moving Target Selection 
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://youtu.be/51XaZDki6yg">[Video]</a>
                            <a class="info" href="https://dl.acm.org/citation.cfm?id=3242608">[DOI]</a>
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            Our previous work has proposed a method to help 2D moving target selection. However, it used a general model trained by all users' data collected. When a new users use our tool, it may be not suitable for him. So we designed a personalized user modeling method to adapt the general model to a specific user.
                        </p>
                    </div>
                </div>

                <hr class="dash">

                <div class="row research-project">

                    <div class="col-md-4">
                        <img src="./Yang Zhang _ Sensing research at CMU_files/research/recognizer.png" width="200">
                    </div>
                    <div class="col-md-8">
                        <h6>
                            Chronos: Improving Recognizers’ Performance by Leveraging Gesture Continuity and Designers’ Involvement
                        </h6>
                        <p class="text-muted">
                            <a class="info" href="https://yangzhang.dev/research/Vibrosight/Vibrosight.pdf">[PDF]</a>
                        </p>
                        <p>
                            We presents Chronos, an algorithm framework that improves the performance of gesture recognizers by 1) extracting the continuity information from gesture sequences and, 2) enabling designers to optimize the decision-making rewards. The framework is implemented by integrating dynamic Bayesian network (DBN) with a partially observable Markov decision process (POMDP).
                        </p>
                    </div>
                </div>


            </div>
            <!-- /left column -->
            <!-- right column -->
            <div class="col-lg-4 mb-2">
                <h2>Latest News</h2>
                <ul class="news">
					<li>Aug 2 Two poster abstracts got conditionally accepted at UIST 2019.</li>
					<li>May 9 Give a talk at CHI 2019 conference.</li>
					<li>Mar 15 One paper got CHI Honorable Mention Award!</li>
					
                </ul>
                <ul class="news" id="news-more">
                    <li>Dec 10 One paper got accepted at CHI 2019.</li>
                </ul>
                <a id="toggle-more-news" href="https://yangzhang.dev/#">More &gt;</a>

            </div>
            <!-- /right column -->
        </div>

        <h2>Fun Life</h2>
        <div class="row fun-projects">
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://github.com/yangz3/DOTA2-Player-Survival-Rate-Analysis">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/1.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/2.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="./Yang Zhang _ Sensing research at CMU_files//life/3.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/4.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/5.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/6.jpg">
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://youtu.be/5Fge4-N5Avo">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/7.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
				<a href="https://youtu.be/H70mu4YS5oo">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/8.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <a href="https://youtu.be/H70mu4YS5oo">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/9.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <a href="https://youtu.be/5Fge4-N5Avo">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/10.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <a href="https://youtu.be/H70mu4YS5oo">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/11.jpg"></a>
            </div>
            <div class="col-lg-2 col-md-3 col-6 mb-3">
                <a href="https://youtu.be/H70mu4YS5oo">
                <img src="./Yang Zhang _ Sensing research at CMU_files/life/12.jpg"></a>
            </div>
        </div>
    </div>

<script src="./Yang Zhang _ Sensing research at CMU_files/jquery.min.js"></script>
<script>
    $('#toggle-more-news').click(function () {
        $('#news-more').toggle();
        $('#news-more').is(':visible') ? $(this).text('< Hide') : $(this).text('More >');
        return false;
    });

    $(window).on('scroll', function () {
        $('video').each(function () {
            var video = $(this)[0];
            var rect = video.getBoundingClientRect();

            if (
                rect.top >= 0 && rect.left >= 0 &&
                rect.bottom <= $(window).height() &&
                rect.right <= $(window).width()
            ) {
                video.play();
            } else {
                video.pause();
            }
        });
    });
</script>

</body></html>